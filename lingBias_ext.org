# -*- coding: utf-8 -*-
# -*- mode: org -*-
#+TITLE: Linguistic Bias in Crowdsourced Articles and the Aspect of Time
#+AUTHOR: Evangelos Mathioudis
#+STARTUP: overview indent inlineimages
#+LANGUAGE: pt-br
#+OPTIONS: H:3 creator:nil timestamp:nil skip:nil toc:nil num:t ^:nil ~:~
#+OPTIONS: author:nil title:nil date:nil
#+TAGS: noexport(n) deprecated(d) ignore(i)
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport

#+LATEX_CLASS: ACMART
#+LATEX_CLASS_OPTIONS: [format=acmsmall, review=false, screen=true]
#+LATEX_CLASS_OPTIONS: [twocolumn, sigconf]
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage{indentfirst}

* ACM-SMALL configuration for org export + ignore tag (Start Here) :noexport:

#+begin_src emacs-lisp :results output :session :exports both
(add-to-list 'load-path ".")
(require 'ox-extra)
(ox-extras-activate '(ignore-headlines))
(add-to-list 'org-latex-classes
             '("ACMART"
               "\\documentclass{acmart}"
               ("\\section{%s}" . "\\section*{%s}")
               ("\\subsection{%s}" . "\\subsection*{%s}")
               ("\\subsubsection{%s}" . "\\subsubsection*{%s}")
               ("\\paragraph{%s}" . "\\paragraph*{%s}")
               ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
#+end_src

#+RESULTS:

* *The Paper*                                                       :ignore:
** Latex configurations                                             :ignore:
#+BEGIN_EXPORT latex
% Metadata Information
\acmConference{Univeristy of Nicosia}{MSc in Data Science}{Project in Data Science}
%\acmVolume{}
%\acmNumber{}
%\acmArticle{}
\acmYear{}
\acmMonth{}
\copyrightyear{Spring, 2021}
%\acmArticleSeq{}
\acmBooktitle{}
\acmPrice{}
\acmISBN{}

% Copyright
\setcopyright{none}
%\setcopyright{none}
%\setcopyright{none}
%\setcopyright{none}
%\setcopyright{none}
%\setcopyright{none}
%\setcopyright{none}

% DOI
\acmDOI{}

% Paper history
%\received{February 2007}
%\received[revised]{March 2009}
%\received[accepted]{June 2009}
#+END_EXPORT
** Frontpage                                                        :ignore:
#+BEGIN_EXPORT latex
\title[Small Title]{Linguistic Bias in Crowdsourced Articles and the Aspect of Time}

\author{Evangelos Mathioudis}
\affiliation{%
  \institution{University of Nicosia}
  \city{Nicosia}
  \country{Cyprus}}
\email{mathioudis.e@live.unic.ac.cy}
#+END_EXPORT

*** Abstract                                                       :ignore:
#+LaTeX: \begin{abstract}
Wikipedia is considered as a reliable and accurate source
of information by many users of the Web. A lot of people will go
through Wikipedia to learn about something they are interested in. In
addition, scientists possibly use Wikipedia as their starting point for their
research by searching on the references for the topic they are
interested in. Wikipedia's content is edited and maintained by
volunteers. With that said, anyone can add or edit the content and
through this express personal or political beliefs. Thus, it is
important for the content to be unbiased from personal beliefs. In
this work, we are going through political biographies and
we are presenting a new approach to detect how linguistic biases have
changed over time. Specifically we compare the level of
subjectivity of the content in biographies about males and females for
7 European politicians. We compare them by using a custom metric the
/Mean Abstract Level/. Based on Linguistic Category Model there are
four levels of abstraction when we describe a person. The /Mean
Abstract Level/ measures the mean level of abstraction in a text. 
#+LaTeX: \end{abstract}
*** ACM Code Classification                                        :ignore:
#+BEGIN_EXPORT latex
%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10002951.10003260</concept_id>
       <concept_desc>Information systems~World Wide Web</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10010405.10010455</concept_id>
       <concept_desc>Applied computing~Law, social and behavioral sciences</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[300]{Information systems~World Wide Web}
\ccsdesc[300]{Applied computing~Law, social and behavioral sciences}
%
% End generated code
%

#+END_EXPORT

*** Keywords, thanks                                               :ignore:
#+BEGIN_EXPORT latex
\keywords{natural language process, wikipedia, sentiment analysis, subjectivity, linguistic bias}
#+END_EXPORT
*** Maketitle                                                      :ignore:
#+LaTeX: \maketitle
** Code                                                             :ignore:
#+begin_src ipython :eval never-export :session session01 :exports none :results none
import numpy as np
import pandas as pd
import json
from utils import frequencies, mean_abstract_level
from tabulate import tabulate

import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
#+end_src

#+begin_src ipython :eval never-export :session session01 :exports none :results none
# Loading the biographies

with open('/home/emath/Data Science/AI Lab-Research/Wikipedia Project/Data/biographies_rev_history_new.json', 'r') as read_file:
    revisions = json.load(read_file)
#+end_src

#+begin_src ipython :eval never-export :session session01 :exports none :results none
# Clear out the revisions that doesn't contain any content
for name in revisions.keys():
    print(name)
    counter = 0 # we keep track of how many revisions per biography have been deleted
    revisions_to_be_deleted = []
    for rev_id in revisions[name].keys():
        if len(revisions[name][rev_id]['content']) == 0:
            # delete all the revisions that don't have content
            # the content has been deleted prior to our analysis
            revisions_to_be_deleted.append(rev_id)
            counter += 1
    for rev_id in revisions_to_be_deleted:
        del revisions[name][rev_id]
#+end_src

#+begin_src ipython :eval never-export :session session01 :exports none :results none
# load the didaxto's dictionaries

with open("/home/emath/Data Science/AI Lab-Research/Wikipedia Project/Data/neg_domain_words.txt", "rb") as read_file:
    neg_domain_words = set(line.decode(errors='ignore').strip() for line in read_file)
    
with open("/home/emath/Data Science/AI Lab-Research/Wikipedia Project/Data/pos_domain_words.txt", "rb") as read_file:
    pos_domain_words = set(line.decode(errors='ignore').strip() for line in read_file)
#+end_src

#+begin_src ipython :eval never-export :session session01 :exports none :results none
# Build lists to create a dataframe
columns = ['name', 'revision Id', 'Date', 'length', 
           'pos_words', 'neg_words', 'adjectives',
           'verbs', 'adverbs'] 
values = []
posTags = {}
for name in revisions.keys():
    i=0
    for revid in revisions[name].keys():
        content = revisions[name][revid]['content']
        pos, neg, adj, verbs, adverbs, length = frequencies(content, posTags, pos_domain_words, neg_domain_words)
        values.append([name, revid, revisions[name][revid]['timestamp'], length, pos, neg, adj, verbs, adverbs])
        i += 1

revisions_df = pd.DataFrame(data=values, columns=columns)
revisions_df['mean_abstract_level'] = mean_abstract_level(revisions_df)
#+end_src

#+begin_src ipython :eval never-export :session session01 :exports none :results raw drawer
revisions_df.info()
#+end_src

#+RESULTS:
:results:
# Out[6]:
:end:

#+begin_src ipython :eval never-export :session session01 :exports none :results none
# create a dataframe. Each row represents a revision for a specific biography
revisions_df['Date'] = pd.to_datetime(revisions_df['Date'])
revisions_df['pos_ratio'] = revisions_df['pos_words']/revisions_df['length']
revisions_df['neg_ratio'] = revisions_df['neg_words']/revisions_df['length']
revisions_df['adj_ratio'] = revisions_df['adjectives']/revisions_df['length']
revisions_df['vb_ratio'] = revisions_df['verbs']/revisions_df['length']
revisions_df['adv_ratio'] = revisions_df['adverbs']/revisions_df['length']

# print("#+CAPTION: Results") 
# print(tabulate(revisions_df.describe()
# , headers='keys', tablefmt='orgtbl', showindex='always'))
#+end_src

#+begin_src ipython :eval never-export :session session01 :exports none :results none
# We replace NaN values in "Mean Abstract Level" column with 0's
revisions_df["mean_abstract_level"] = revisions_df["mean_abstract_level"].fillna(0)

def gender(row: str):
    """
    Goes through all the names and returns 'm' if
    the name belong to a male or 'f' otherwise.
    """
    if row in  ["Recep Tayyip ErdoÄŸan", "Emmanuel Macron", "Kyriakos Mitsotakis"]:
        return "m"
    else:
        return "f"

# Create a new column "Gender"
revisions_df["gender"] = revisions_df["name"].apply(gender)
#+end_src

#+begin_src ipython :eval never-export :session session01 :exports none :results none
revisions_df.groupby("gender").agg({"length":"mean", "mean_abstract_level":"mean"})

print("#+CAPTION: Results") 
print(tabulate(revisions_df.groupby("gender").agg({"length":"mean", "mean_abstract_level":"mean"})
, headers='keys', tablefmt='orgtbl', showindex='always'))
#+end_src
** Introduction
\label{sec.introduction}
Wikipedia is a source of information for many users on the
web. Additionally, a large number of researchers of various domains
are using Wikipedia as a starting point for their research. But, Wikipedia
is an open-collaboration encyclopedia and everyone is free to add
content and express their personal opinion on the context that is
described. For this reason, Wikipedia's
article content needs to be unbiased from personal statements and
experiences. 

Personal statements and experiences are expressed through linguistic
biases. /Linguistic biases/ come in two different ways. The
/Linguistic Expectancy Bias/ or LEB is referred to how we communicate
information related to a person or an event based on what our
expectations of these are. We tend to use more abstract language when we
talk about something consistent with our expectations
cite:20.500.12613/1481,otterbacher19_linguis_bias_crowd_biogr. The
/Linguistic Intergroup Bias/ or /LIB/ describes how we express
ourselves regarding actions and atributes of people who belong to our
social group. Our expectations suggest that in-group members act positively, thus
we are going to talk about the positive actions and characteristics
more descriptively. On the other hand,  we tend
to describe a negative behavior with less information and more
concretely cite:hippel97_linguis_inter_bias_as_implic_indic_prejud,otterbacher19_linguis_bias_crowd_biogr.
*** Motivation
Our work is based on previous work of
citeauthor:otterbacher19_linguis_bias_crowd_biogr. We have focused our
research on how linguistic biases on biographies about politicians
have changed over time. By using the Wikipedia API MediaWiki we
downloaded the histories of revisions for several biographies about
politicians.  Additionally, we have used the tool /Didaxto/[fn:1] to
extract domain-specific dictionaries and we have analyzed
the biographies concerning the number of verbs, adverbs, and
adjectives they contain. Didaxto implements an unsupervised learning
approach to detect patterns and extract domain specific dictionaries.

** Related Work
\label{sec.relatedwork}
Due to its collaboratively structure, Wikipedia has attracted the interest of
scientists and researchers from different domains. Wikipedia's
content has been the object of research related to bias from different
perspectives.
*** Gender-Race Bias
citeauthor:worku20_explor_system_bias_artic_delet investigates
if content that is preferred by females has greater chances of
deletion due to the imbalanced number of male and female
contributors. A different approach is presented in
cite:beytia20_posit_matter. This work examines the
inequality of geographical coverage on biographies of notable
individuals. The results reveal that it is clear that Wikipedia
contains more articles related to the Global North and the Traditional
Western powers cite:beytia20_posit_matter.
*** Linguistic Bias
citeauthor:otterbacher19_linguis_bias_crowd_biogr focuses on the linguistic bias
on biographies of notable individuals concerning gender and
ethnicity on three different locations, the whole content, the first
paragraph, and on the knowledge panel on the search engine results
page. Besides that, it compares the result across the English and Greek
Wikipedias. The conclusion indicates that there are linguistic biases both across
Wikipedias and in each separately.
*** Prejudice
In this work cite:umarova19_how_partis_perceiv_polit_bias, the idea is
to correctly assign political labels to news publishers in
Wikipedia. The reason is that most of the users when searching on the Web will take a
quick look at the knowledge panels of the search engine result pages
to get informed. Wikipedia's content is used on third-party information providers, thus it is
important to have been verified before.
** Methodology
\label{sec.methodology}
*** Overview 
\label{sec.overview}
For many people Wikipedia is the main source of reliable information, providing
a significantly large number of subjects. One of the most popular
categories is the biographies of notable individuals. The idea for this
work is to specifically research the linguistic biases that occur in
politicians' biographies and their effect on the subjectivity of each
biography. Based on the /Linguistic Category Model/
cite:seminil_linguis_categ_model there are four predicates
categories that our words might fell in. These categories are depicted
in Figure ref:LCM-predicates
#+CAPTION: The Four Predicates based on LCM
#+label: LCM-predicates
[[./images/lcm_predicates.jpg]]

Moving from bottom to top, we describe a person more concretely when
we use more descriptive action verbs. On the other hand, we describe a
person in a more abstract way if we use more adjectives. 
*** Dataset Description and Analysis
\label{sec.eda}
**** Dataset
We have built our dataset by using Wikipedia's API MediaWiki. MediaWiki
gives you access to a lot of features and one of them is the history
of edits for each biography. The form of the data that Wikipedia
returns when someone requests a specific page is on
/wikitext/. We parsed the edits using Python's library
MediaWiki Parser From Hell[fn:2]. We chose to use history of edits for 7
politicians from Europe. For each edit we have kept track of the *revisionId*,
*timestamp*, *userId*, *content* and *tags*.

We will call each edit a *snapshot* of the biography for a specific
time *$t_i$*. A very simplistic way to draw someone the history of edits should
look like is the Figure ref:figure-1  below.
#+caption: Snapshots of biography.
#+label: figure-1
[[./images/snapshots.png]]
It is worth noticing that
*$snapshot_i$* is not necessarily different from *$snapshot_{i+1}$*. As
have been stated in
cite:chhabra20_dynam_edit_war_sequen_wikip there are some special kind of
edits called /Reverts/. /Revert/ is the case when the *$user_n$* does some
edits at the time *$t_n$* and afterwards another user let's say *$user_m$*
reverts the biography to the snapshot of time *$t_{n-1}$* or even prior
i.e *$t_{n-k}$*. We aren't going deeper into this, but this is a phenomenon
called *Edit Wars* cite:chhabra20_dynam_edit_war_sequen_wikip and we
have observed a lot of instances of these
biographies. So, for each *snapshot* we have got some metadata and the
content. There were cases that the content was /hidden/ and these cases
we chose to drop but it could be a future research question.
**** Exploratory Data Analysis
We fed the contents of the biographies to Didaxto and we have extracted
domain-specific dictionaries with positives and negatives
words. After that we plugged each of these words in the module
/nltk.pos_tag()/[fn:3] and have kept the part of speech that
represents. To create a metric of subjectivity we wanted to identify
the four predicators that we came up with to the /Linguistic Category
Model/ which are Descriptive Action Verbs (DAV), Interpretative Action Verbs (IAV),
State Verbs (SV), and Adjectives (ADJ). We have inspired our formula from
the /mean abstract level/ formula from
cite:h.05_when_do_we_commun_stereot. Unfortunately, the /NLTK
POS-tagger/ is not able to characterize the words this way, thus we
picked everything related to verbs, adverbs, and adjectives. 
\begin{equation} \label{mean_abstract_level}
MAL = \frac{2*numOfVBs+4*(numOfAVBs + numOfADJs)}{numOfVBs + numOfAVBs + numOfADJs}
\end{equation}
As can be seen in Eq. [[eqref:mean_abstract_level][Mean Abstract Level]]  we gave a
weight of 2 on the verbs and a weight of 4 on adverbs and
adjectives. And then we have divided by the sum of all occurrences.
A $MAL=2$ means that there are no evidence of linguistic biases
and a $MAL=4$ means that the content is biased.

In addition, for each *snapshot* we have calculated the relative
frequencies of positive and negative words. Also, we have extracted the relative
frequencies for adjectives, verbs and adverbs. Furthermore, we have created a
continuous variable named /mean_abstract_level/ and a feature called
/gender/ to be able to do aggregations based on gender and research
whether there are linguistic biases related to the gender of
politicians.

We have noticed that some days were very "busy" and there was a lot of
"action" happening in very short intervals. For that reason, we
resample our dataset and have kept the last measure for each of the
above measurements. We have concluded to a dataset that consists of
17721 rows, each one represents a snapshot of a specific
biography[fn:4]. Table ref:table-1 compares the average values of the
/length of biography/ and the /mal score/ for males and females politicians.
#+begin_src ipython :eval never-export :session session01 :exports none :results none :ipyfile ./images/female_politicians.png
female_politicians = revisions_df.loc[revisions_df["gender"]=="f"]
# print("#+CAPTION: Results") 
# print(tabulate(female_politicians[["length", "adjectives", "verbs", "adverbs", "mean_abstract_level"]].describe()
# , headers='keys', tablefmt='orgtbl', showindex='always'))

# Plots for densities of Length and MAL
# fig = plt.figure(figsize=(8,6))
# fig.subplots_adjust(top=0.85, wspace=0.3)
# fig.suptitle('Length and MAL Distributions')

# ax = fig.add_subplot(111)
# ax.tick_params(axis='both', which='major', labelsize=8.5)
# ax.spines["top"].set_color("None")
# ax.spines["right"].set_color("None")
# # sns.kdeplot(female_politicians["length"], ax=ax, shade=True, label="Length")
# sns.kdeplot(female_politicians["mean_abstract_level"], ax=ax, shade=True, label="MAL")
# plt.legend();
#+end_src
#+begin_src ipython :eval never-export :session session01 :exports none :results none
male_politicians = revisions_df.loc[revisions_df["gender"]=="m"]
print("#+CAPTION: Results") 
print(tabulate(male_politicians[["length", "adjectives", "verbs", "adverbs", "mean_abstract_level"]].describe()
, headers='keys', tablefmt='orgtbl', showindex='always'))
#+end_src
#+begin_src ipython :eval never-export :session session01 :exports none :results none
revisions_df.groupby("gender").agg({"length":"mean", "mean_abstract_level":"mean"})

print("#+CAPTION: Results") 
print(tabulate(revisions_df.groupby("gender").agg({"length":"mean", "mean_abstract_level":"mean"})
, headers='keys', tablefmt='orgtbl', showindex='always'))
#+end_src
#+ATTR_LATEX: :mode table :align
#+caption: label:table-1
| gender   |   length |   mean_abstract_level |
|----------+----------+-----------------------|
| f        |  4656.6  |               2.09278 |
| m        |  8904.95 |               2.08482 |
We observe slightly different /mean_abstract_level scores/
which we would like to investigate if it could be by chance or
not. Thus, we applied a hypothesis test. Our hypothesis are:
#+begin_quote
/Null Hypothesis $H_0$: The average of female MAL score is equal to
average of male MAL/

/Alternative Hypothesis $H_1$: The average of female MAL score is
higher than the average of male MAL/
#+end_quote
Because our entries represent an event that happened to a specific
time *$t_n$*, we assume that each one of these entries is a single
article in Wikipedia. This assumption is helping us to be able to
apply /T-test/ on data that represent time series data.
#+begin_src ipython :session session01 :exports none :eval never-export :results none
from scipy.stats import ttest_ind
def t_test(x,y,alternative='both-sided'):
            t, double_p = ttest_ind(x,y,equal_var = False)
            if alternative == 'both-sided':
                pval = double_p
            elif alternative == 'greater':
                if np.mean(x) > np.mean(y):
                    pval = double_p/2.
                else:
                    pval = 1.0 - double_p/2.
            elif alternative == 'less':
                if np.mean(x) < np.mean(y):
                    pval = double_p/2.
                else:
                    pval = 1.0 - double_p/2.
            return t, pval

t_statistic, p_value = t_test(female_politicians["mean_abstract_level"], male_politicians["mean_abstract_level"], alternative="greater")
df = pd.DataFrame(data={"T-statistic":[t_statistic], "p-Value":[p_value]})
print("#+CAPTION: Results") 
print(tabulate(df, headers='keys', tablefmt='orgtbl', showindex='always'))
#+end_src
#+ATTR_LATEX: :mode table :align
#+caption: label:T-test Results
|    |   T-statistic |     p-Value |
|----+---------------+-------------|
|  0 |       4.06077 | 2.45858e-05 |

Table ref:T-test shows the results of our hypothesis test. For a
confidence level of 95% we get a $p\asymp0.00002$. We conclude that there
are evidences to reject the null hypothesis.
**** Connections to Real Events
#+begin_src ipython :session session01 :exports none :results none :eval never-export
revisions_df.info()
#+end_src
#+begin_src ipython :session session01 :exports none :results none :eval never-export
revisions_df_copy = revisions_df.copy()
revisions_df_copy.set_index("Date").groupby("name").resample("D").agg({"length": "last", "pos_ratio":"last", "neg_ratio":"last", "mean_abstract_level":"last"})
erdogan = revisions_df_copy.set_index("name").loc["Recep Tayyip ErdoÄŸan"]
#+end_src
#+begin_src ipython :eval never-export :session session01 :exports none :results none :ipyfile ./images/erdogan_weekly_mal.png
erdogan_weekly = erdogan.set_index("Date").resample("W").agg({"length": "last", "pos_ratio":"last", "neg_ratio":"last", "mean_abstract_level":"last"})

def normalize(arr, t_min=0, t_max=1):
    norm_arr = []
    diff = t_max - t_min
    diff_arr = max(arr) - min(arr)    
    for i in arr:
        temp = (((i - min(arr))*diff)/diff_arr) + t_min
        norm_arr.append(temp)
    return norm_arr

erdogan_weekly["mal_norm"] = normalize(erdogan_weekly["mean_abstract_level"])
erdogan_weekly["posRatio_norm"] = normalize(erdogan_weekly["pos_ratio"])
erdogan_weekly["negRatio_norm"] = normalize(erdogan_weekly["neg_ratio"])

fig = plt.figure(figsize=(14,10))
fig.subplots_adjust(top=0.85, wspace=0.3)
# fig.suptitle('Test plot')
ax = fig.add_subplot(2,1,1)

tmp_df = erdogan_weekly.loc[erdogan_weekly["mal_norm"]>0]
ax.plot(tmp_df['mal_norm'], '-r', label="Mean Abstract Level")
ax.plot(tmp_df["posRatio_norm"], '-k', label="Positive Words")
ax.plot(tmp_df["negRatio_norm"], "-y", label="Negative Words")

ax.spines["top"].set_color("None")
ax.spines["right"].set_color("None")
    
plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
         rotation_mode="anchor")

plt.suptitle("Recep Tayyip ErdoÄŸan")
plt.legend()
#+end_src
#+begin_src ipython :session session01 :exports none :results none :eval never-export
revisions_df_copy = revisions_df.copy()
revisions_df_copy.set_index("Date").groupby("name").resample("D").agg({"length": "last", "pos_ratio":"last", "neg_ratio":"last", "mean_abstract_level":"last"})
merkel = revisions_df_copy.set_index("name").loc["Angela Merkel"]
#+end_src
#+begin_src ipython :eval never-export :session session01 :exports none :results none :ipyfile ./images/merkel_weekly_mal.png
merkel_weekly = merkel.set_index("Date").resample("W").agg({"length": "last", "pos_ratio":"last", "neg_ratio":"last", "mean_abstract_level":"last"})

def normalize(arr, t_min=0, t_max=1):
    norm_arr = []
    diff = t_max - t_min
    diff_arr = max(arr) - min(arr)    
    for i in arr:
        temp = (((i - min(arr))*diff)/diff_arr) + t_min
        norm_arr.append(temp)
    return norm_arr

merkel_weekly["mal_norm"] = normalize(merkel_weekly["mean_abstract_level"])
merkel_weekly["posRatio_norm"] = normalize(merkel_weekly["pos_ratio"])
merkel_weekly["negRatio_norm"] = normalize(merkel_weekly["neg_ratio"])

fig = plt.figure(figsize=(14,10))
fig.subplots_adjust(top=0.85, wspace=0.3)
# fig.suptitle('Test plot')
ax = fig.add_subplot(2,1,1)

tmp_df = merkel_weekly.loc[merkel_weekly["mal_norm"]>0]
ax.plot(tmp_df['mal_norm'], '-r', label="Mean Abstract Level")
ax.plot(tmp_df["posRatio_norm"], '-k', label="Positive Words")
ax.plot(tmp_df["negRatio_norm"], "-y", label="Negative Words")

ax.spines["top"].set_color("None")
ax.spines["right"].set_color("None")
    
plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
         rotation_mode="anchor")

plt.suptitle("Angela Merkel")
plt.legend()
#+end_src
It is interesting to see how do politicians' action affect the world's
opinion and if this can be identified on social media or in
Wikipedia's articles. As have been observed in
cite:agarwal20_wikip_westm there is a lot of attention on political
biographies during election times related to signs of engagement on
other social media.

In Figure ref:label:figure-2 we plot the normalized values of the /mean abstract
level/, and the relative frequencies of positive and negative
words of /Recep Tayyip ErdoÄŸan/'s[fn:5] biography.
#+caption: Erdogan's Weekly MAL Scores, positive and negative words concetrations
#+label: label:figure-2
[[./images/erdogan_weekly_mal.png]]
We are going to give some highlights of this plot:
1. There are picks for positive and negative concentrations after the
   elections on 2007[fn:10] 
2. In the summer of 2008, the Turkish prime minister visited Baghdad
   after some period of tense between two countries[fn:6].
3. A big drop of positives words at the beginning of 2009 is followed by an increase in the MAL score. These changes could be related to
   Erdogan's meeting with the Israeli president on Davos[fn:7].
4. After 2009, the mean abstract level is increasing for about 4
   years. The positive and negative words frequencies at the same
   period follow this increase but at a smaller rate. There is a pick
   for /mal score/ which could correlate with an award that the
   Turkish prime minister received[fn:8].
5. In the summer of 2013 CNN posted an article[fn:9] with the title "Turkey's
   Erdogan: Successful leader or 'dictator'?".

#+CAPTION:  Merkel's Weekly MAL Scores, positive and negative words concetrations
#+label: figure-3
[[./images/merkel_weekly_mal.png]]
On the other hand, in Figure ref:figure-3 we have plotted the weekly
changes in /Mean Abstract Level/ and the changes in concentration of
positive and negative words for Merkel's biography. We highlight the
following:
1. It is clear that Merkel as a woman is being described more
   abstractly than Erdogan.
2. The subjectivity remains stable from the beginning.
3. Positive and negative words have almost the same increase for the
   first one and a half years.
4. In May 2005, Merkel won the national elections.
5. It is almost after the national elections of 2005 until now that
   positive words dominate negative words in the biography.

/It is important here to clarify that all the above haven't verified
quantitatively but they are the results of the author's research. The
author picked these two politicians because they had the largest
numbers of revisions./
** Discussion
We would like to focus a bit more to figure ref:label:figure-2. The
/mal scores/ for all biographies presented several up and downs. But
in each case, there was always a plateau. Wikipedia is an open online
encyclopedia and everyone can contribute to the content. It is obvious that the maintainers of
Wikipedia isn't able to manage this volume of content. As a result,
polarized volunteers have the chance to contribute positive or negative
cite:agarwal20_wikip_westm,umarova19_how_partis_perceiv_polit_bias
depending on the content and in line with the /LEB/ and /LIB/ 
models we have already talked about.

The same holds and in the case of /Erdogan's/ biography. We observe
that at its birth the biography is less subjective. Over years, more
polarized contributors add their personal opinion to the
biography. This goes on for several years when the more neutral
authors as per the subjectivity take action. One thing that must not
be confused about is that subjectivity is possible to be expressed with
both positive and negative words. That means when the subjectivity
increases doesn't mean that the content is in favor of the person of interest. 
** Conclusion
Ultimately, we conclude on this, our /t-test/ has given us evidence
that there are linguistic biases on political biographies through
time. We have also plotted two biographies of different genders. The
comparison of these plots confirms our results in t-test. Of course,
we have focused only on a small number of European politicians and
this would be the object of future work. Also, we find
possible correlations of changes on the subjectivity and real events
that relate to each politician. This of course could be an object for
more research.
* Bib file is here                                                   :ignore:
bibliographystyle:ACM-Reference-Format
bibliography:./references.bib
* Emacs setup                                                      :noexport:

* Footnotes
[fn:10]https://www.forbes.com/2007/07/23/turkey-erdogan-elections-biz-cx_0724oxford.html 

[fn:9] https://edition.cnn.com/2013/06/04/world/europe/turkey-erdogan/index.html 
[fn:8] https://news.un.org/en/story/2010/03/330952-turkish-prime-minister-wins-first-ever-un-award-memory-slain-lebanese-leader 
[fn:7] https://www.theguardian.com/world/2009/jan/30/turkish-prime-minister-gaza-davos 
[fn:6] https://www.nytimes.com/2008/07/11/world/middleeast/11iraq.html
[fn:5] Recep Tayyip ErdoÄŸan https://en.wikipedia.org/wiki/Recep_Tayyip_Erdo%C4%9Fan 
[fn:4] All the code will be uploaded on  https://git.io/JOc7Z
[fn:3] Nltk POS-tagger https://www.nltk.org/book/ch05.html 
[fn:2] mwparserfromhell https://pypi.org/project/mwparserfromhell/ 
[fn:1] Didaxto http://deixto.com/didaxto/ 
# Local Variables:
# eval: (add-to-list 'load-path ".")
# eval: (require 'ox-extra)
# eval: (ox-extras-activate '(ignore-headlines))
# eval: (add-to-list 'org-latex-classes '("ACMART" "\\documentclass{acmart}" ("\\section{%s}" . "\\section*{%s}") ("\\subsection{%s}" . "\\subsection*{%s}") ("\\subsubsection{%s}" . "\\subsubsection*{%s}") ("\\paragraph{%s}" . "\\paragraph*{%s}") ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
# End:

